\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces 機械学習の一般的プロセス}}{2}
\contentsline {figure}{\numberline {1.2}{\ignorespaces 深層学習で画像を認識する流れ}}{3}
\contentsline {figure}{\numberline {1.3}{\ignorespaces 深層学習による、顔の構成要素の学習結果}}{4}
\contentsline {figure}{\numberline {1.4}{\ignorespaces 猫を認識するニューロン (教師無しYoutubeビデオより学習された。)}}{5}
\contentsline {figure}{\numberline {1.5}{\ignorespaces 上 : 入力画像の中で、ニューロンが最も強く反応した48枚　下 : 計算上、最もニューロンが強く反応する画像}}{5}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces リンク予測の問題設定}}{9}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Stanford大が製作した、深層学習による感情分析の例}}{10}
\contentsline {figure}{\numberline {2.3}{\ignorespaces サポートベクターマシンのマージン最大化}}{12}
\contentsline {figure}{\numberline {2.4}{\ignorespaces 直線にて分類できない場合}}{12}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Imagenetの画像とラベル構造の例}}{16}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Supervisionによる画像分類の結果の例}}{16}
\contentsline {figure}{\numberline {3.3}{\ignorespaces DeViSEの学習方法}}{17}
\contentsline {figure}{\numberline {3.4}{\ignorespaces 深層信念ネットワークworkの構造と、学習過程}}{18}
\contentsline {figure}{\numberline {3.5}{\ignorespaces Restriceted Boltzman Machineのネットワーク構造}}{19}
\contentsline {figure}{\numberline {3.6}{\ignorespaces 自己符号器の構造図}}{21}
\contentsline {figure}{\numberline {3.7}{\ignorespaces 画像に対する雑音除去自己符号器}}{22}
\contentsline {figure}{\numberline {3.8}{\ignorespaces 雑音除去自己符号器の模式図}}{22}
\contentsline {figure}{\numberline {3.9}{\ignorespaces 積層雑音除去自己符号器における学習の進行}}{23}
\contentsline {figure}{\numberline {3.10}{\ignorespaces 畳み込みネットワークの仕組み}}{23}
\contentsline {figure}{\numberline {3.11}{\ignorespaces 畳み込みネットワークと素性写像}}{25}
\contentsline {figure}{\numberline {3.12}{\ignorespaces Maxpoolingと、Maxpooling レイヤーの作用}}{25}
\contentsline {figure}{\numberline {3.13}{\ignorespaces 最大値蓄積レイヤーを取り入れた畳み込みネットワーク}}{25}
\contentsline {figure}{\numberline {3.14}{\ignorespaces DropoutによるTIMITデータベースの識別結果}}{26}
\contentsline {figure}{\numberline {3.15}{\ignorespaces DropConnectの模式図}}{27}
\contentsline {figure}{\numberline {3.16}{\ignorespaces ヘビサイドの階段関数}}{27}
\contentsline {figure}{\numberline {3.17}{\ignorespaces シグモイド関数}}{28}
\contentsline {figure}{\numberline {3.18}{\ignorespaces 整流関数}}{29}
\contentsline {figure}{\numberline {3.19}{\ignorespaces Maxout Networkの構造図}}{30}
\contentsline {figure}{\numberline {3.20}{\ignorespaces Maxout ユニットが凸関数を近似する様子(1次元の場合)}}{30}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces 既存コードの利用における、完全オリジナルコードの作成に対する利点}}{32}
\contentsline {figure}{\numberline {4.2}{\ignorespaces 既存ソースコード利用のリスクと、対処方法}}{32}
\contentsline {figure}{\numberline {4.3}{\ignorespaces Theanoの動作過程}}{35}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Pylearn2の実験計画例}}{37}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces MNISTデータの例}}{40}
\contentsline {figure}{\numberline {5.2}{\ignorespaces CIFAR10データの例}}{42}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Maxout NetworkによるMNIST分類実験で、誤差が論文より大きくなった原因}}{46}
\addvspace {10\p@ }
