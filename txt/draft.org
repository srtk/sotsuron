
* 概要
近年機械学習の分野において、Deep Learningと呼ばれるアルゴリズム群が優れた成果を納めている。Web工学でも、Deep Learningを応用することによる発展が期待される。
しかし、Deep Learningは歴史の浅い発展途上の技術であり、どのアルゴリズムを定番とすれば良いのか、試行錯誤の段階にある。これは、各アルゴリズムの改良点が次々と見つかっていることに加え、各アルゴリズムの得手不得手や、学習性能が高くなる原理の詳細など、解明されていない部分が多いことが、主な原因である。アルゴリズムが開発途上で確定できていないため、公開されているライブラリも、現状では、開発用途や実験的なものが多くなってしまっている。そもそも有力なアルゴリズムに対応する実装が用意されていない場合や、問題に応じて自らアルゴリズムの細部を調整しなければならない場合もある。標準と言える公開ライブラリが確立していない状況なので、Web工学など応用分野にDeep Learningを適用したいと考えても、プログラム開発に長い時間がかかってしまい、開発における大きな障壁となっている。
さらに、現在のDeep Learning技術では、他のアルゴリズムに比べて学習にかかる時間が長いことが多く、ハードウェア性能が低いマシンでは、アルゴリズムを実用的な時間で実行すること自体が容易ではない。実行時間の長さをカバーするため、GPUを用いて演算をスピードアップさせる手法が確立されつつあるが、特殊なプログラミングが要求され、開発における障壁の1つとなっている。また、ノートPCの大部分など、並列演算に利用可能なGPUを搭載していないPCを使っている場合には、ライブラリがGPUを利用しているために、却ってその実行が不可能になってしまうこともある。
以上に挙げた原因により、Deep Learning技術に関心を持っていても、まず実際の問題にDeep Learningを試行すること自体が困難であり、応用技術開発のハードルは更に高くなっている。特に、国内での研究開発は遅れており、早急なキャッチアップが必要である。このような現状を踏まえ、本研究では、Web工学における応用を見据えつつ、Deep Learningを様々な問題に応用するための方法論を整理する。具体的な目標としては、Deep Learningの特徴である高い学習性能を確実に利用できて、その上で出来る限り、実行時間の短さ、実行プログラムの使いやすさ、アルゴリズムの調整・改良の容易さを兼ね備えた方法を確立する。

* 第1章　はじめに
** 1.1 Web工学と機械学習
近年Web工学の領域においては、広告の効率アップや文章の分析、検索エンジンの性能向上などが求められている。このため、ユーザの性格や行動の分析、ネットワーク構造の未来予測、文章の意味や感情の理解といった技術が必要とされている。
例えば、Web上で物を販売しているサイトにおいて、ユーザが購入しそうな商品を勧め、売り上げを増大させることを考える。ユーザが今まで見た商品、購入した商品を分析して、ユーザの求めている商品のジャンルを知ったり、Webショッピングにおける性格を知ることが出来れば、ユーザが買いたい商品を先回りして、広告表示することが出来る。ユーザに対して、より購入してくれそうな広告を多く見せて、広告の効率をアップさせることで、サイトからの収益が上がることが期待できる。
Web工学では、Web上のデータを元に知識を学習し、次に得た知識を用いて予測や分類を行う、というアルゴリズムが必要になっている。例えば、ユーザ行動やソーシャルネットワーク構造の傾向を学習して、将来の動向を予測したり、ユーザに送られたメールの意味を理解できるようになって、迷惑メールとそうでないものを分類できるようになる、といった具合である。こういった知識学習のためには、機械学習(マシンラーニング)と呼ばれる手法が広く用いられている。
機械学習とは、対象となる知識を数学的なモデルで表し、より適切なモデルを求めて微修正を重ねることで、知識を洗練させていくアプローチである。使用する数学的なモデルの大筋は、経験を基にあらかじめ決めておく。ユーザ行動、リンク構造、メールの文章といった元データは、全て数値に変換されて、モデルに入力される。モデルは、入力データから予測や識別を実際に行いつつ、より良い結果が得られるように、モデル内の係数を調整していく。最終的に、多種多様なデータに対して、適切な予測や識別を行うことが出来る数学的モデルを得ることができるのである。
** 1.2 Deep Learningの台頭
機械学習における大きなポイントの1つに、元データをどのような数値データに変換するか、という問題がある。
変換された数値データのことを、 特徴量、あるいは素性(feature)と呼ぶ。機械学習は、「生データから素性への変換」「素性をモデルに入力」「モデルによる数値計算で、結果を出力」「結果がより正確になるよう、モデルを修正」というプロセスの繰り返しで成り立っている。このうち、「素性への変換」だけは、画像・文章・リンク構造といった、データの種類に依存している。いったん素性が数値の形で得られれば、残りのプロセスは、データの種類に依存せず、全て入力数値とモデルの関係だけで解決することができる。つまり、機械学習のプロセスは、データの種類に依存する素性変換と、汎用的に使えるモデル学習の部分に分かれているのである。
機械学習の性能を上げるためには、素性への変換部分を工夫する方法と、使用する数学的モデルを洗練する方法の2つがある。素性への変換部分の改良は、対象となるデータの種類に強く依存している。例えば、画像データを素性数値に変換する場合、単純なRGB画素データをそのまま使っても良いが、SIFT特徴量やSURF特徴量、フィッシャーベクトルなど、より画像の特徴を捉えた特徴量を用いることが定石となっている。音声データに対しては、時間領域や周波数領域の波形をそのまま用いても良いが、メル周波数ケプストラム係数と呼ばれる特徴量も用いられる。問題とデータ種別に応じて、利用する特徴量を工夫することにより、機械学習の識別精度や実行時間などの性能が向上することが知られていた。このとき、特徴量はそれぞれのデータの専門家による、謂わば職人芸によって作られていた。
2000年代半ばに、Deep Learningと呼ばれる一群の手法が台頭した。Deep Learningとは機械学習の手法の1つで、人間の脳の構造に類似した、多層ニューラルネットワーク構造をモデルに用いて、学習を行わせる方法である。Deep Learningは、画像認識や音声認識、化合物の生成予測といった分野で優れた性能を示し、注目を浴びるようになった。
Deep Learningの特徴の一つとして、生の入力データから自動的に素性を作り、抽象表現を習得する働きがあると考えられている。例えば人間の顔画像データを学習させた場合、多層ニューラルネットワークを構成する複数のレイヤーのうち、入力に近い低レイヤーのニューロンは、画像のエッジ部分に対して強く反応し、出力に近い高レイヤーのニューロンでは、目口鼻、さらに顔全体など、より抽象度の高い要素に対して反応することがわかった。これは、見方を変えれば、低レイヤーでは中間表現(素性にあたる)を抽出しており、低レイヤーで得た素性を高レイヤーに入力することにより、より高レベルな抽象的概念に対しても優れた識別精度を実現している、と考えられる。どのような素性を使えば良い結果が出るのか、素性の抽出方法自体を同時に機械学習していることから、Deep LearningはRepresentation Learning(表現学習)とも呼ばれている。
** 1.3 Deep Learningの課題と、研究の目的
Deep Learningが高い識別性能を持つことがわかり、Deep Learningを身近な問題に適用して、良い成果を得たいという機運が高まっている。例えば、Web工学の分野では機械学習が大きな役割を果たしており、この学習プロセスにDeep Learningを組みこむことで、学習精度が向上したり、より多様な情報を扱えるようになる可能性がある。出来るだけ簡易に、Deep Learningを様々な問題に応用するための方法論が求められている。
しかし、Deep Learningは歴史の浅い発展途上の技術であり、どのアルゴリズムを定番とすれば良いのか、試行錯誤の段階にある。これは、各アルゴリズムの改良点が次々と見つかっていることに加え、学習性能が高くなる原理や、各アルゴリズムの得手不得手など、解明されていない部分が多いことが、主な原因である。アルゴリズムが開発途上で確定できていないため、公開されているライブラリも、現状では、開発用途や実験的なものが多くなってしまっている。実験的なライブラリでは、一部の種類のデータにのみ適用されることを想定して書いている場合があり、他の種類のデータを扱うためには、データ変換用のソースコードを記述しなければならないケースが多い。そもそも有力なアルゴリズムに対応する実装が公開されていない場合もあり、この場合、アルゴリズムの部分も含めて全ての実装を用意しなければならない。また、問題に応じて自らアルゴリズムの細部を調整しなければならない場合もある。例えば、学習の繰り返し回数(エポック数)や、どの種類のレイヤーを何回重ねるべきか(レイヤー構造)、1つのレイヤーに含まれる学習素子(ニューロン)の個数はどうするか、などである。これらはソースコードの作成者が経験的に手作業で調整しているケースが多く、標準と言える公開ライブラリが確立していない状況なので、Web工学など応用分野にDeep Learningを適用したいと考えても、プログラム開発に長い時間がかかってしまう。開発における大きな障壁となっている。
さらに、現在のDeep Learning技術では、他のアルゴリズムに比べて学習にかかる時間が長いことが多く、ハードウェア性能が低いマシンでは、アルゴリズムを実用的な時間で実行すること自体が容易ではない。実行時間の長さをカバーするため、GPUを用いて演算をスピードアップさせる手法が確立されつつあるが、特殊なプログラミングが要求され、開発における障壁の1つとなっている。また、ノートPCの大部分など、並列演算に利用可能なGPUを搭載していないPCを使っている場合には、ライブラリがGPUを利用しているために、却ってその実行が不可能になってしまうこともある。
以上に挙げた原因により、Deep Learning技術に関心を持っていても、まず実際の問題にDeep Learningを試行すること自体が困難であり、応用技術開発のハードルは更に高くなっている。特に、国内での研究開発は遅れており、早急なキャッチアップが必要である。このような現状を踏まえ、本研究では、Web工学における応用を見据えつつ、Deep Learningを様々な問題に応用するための方法論を整理する。Deep Learningの特徴である高い学習性能を確実に利用できて、その上で出来る限り、実行時間の短さ、実行プログラムの使いやすさ、アルゴリズムの調整・改良の容易さを兼ね備えた方法を確立する。
Deep Learningアルゴリズムのベンチマークを取ることで、Deep Learningを応用する際のノウハウを集積する。 
* 第2章　関連研究
** 2.1 Web工学と機械学習
この項では、Web工学における課題をいくつか例示し、それらの課題を解決するために、機械学習がどのように用いられてきたかを概観する。
2.1.1 Recommendation System
主にWebショッピングを含むサイトや、Web広告の配信において求められる技術である。Webサイトを閲覧しているユーザに対し、そのユーザが購入したいと商品を予測して、Web上の広告などの形で推薦する。適切な広告を表示することにより、ユーザの購買行動を促進することが出来る。
Recommendation Systemの研究は、1992年のTapestryシステムに始まる[Goldberg 92]。また、少なくとも90年代の終わりには、Amazon.com, CDNOW, eBay, Levis, GroupLensなど、様々なサイトにてRecommendation Systemは利用されていた[Resnick 97] [Schafer 99]。
Recommendation Systemを実現するための、機械学習のテクニックとしては、大きく2種類が挙げられる[Koren 2009]。1つは、Content Filteringと呼ばれ、ユーザや商品の属性や購買傾向を学習することことで、推薦を行う。もう1つはCollaboraitive Filteringと呼ばれており、ユーザや商品の属性を扱う代わりに、購入や評価といった、ユーザの過去の行動を基にして、推薦を行う。
2.1.2 Link Prediction
グラフにおいて、現在のノード間接続から未来の接続を予測する
ソーシャルグラフの予測、タンパク質の反応予測(PPI)に有効である
[加筆]
2.1.3 Sentiment Analysis
ユーザの感想データを手に入れられるようになってきた
事実だけでなく、他の人がどのような感情を抱いているのかを分析したい
opinion miningという語も広義には似た内容を指している
2001-2003に研究が出始めた
common neighbors
Jaccard係数 SVDによる次元削減
Stanfordによるデモに言及
[加筆]
2.1.4 Learning to Rank
多様なランキング素因を組み合わせて、ランキング関数を作成
→どの組み合わせが有効か、機械学習する
[加筆]
** 2.2 機械学習で利用される、代表的な分類器
機械学習のプロセスは、「入力データを、数学的モデルで使える素性に変換する」「素性を数学的モデルに入力して、出力値を得る」「出力を見ながら、モデルを修正する」という行程に大きく分けられる。データの分類問題を機械学習で解く場合、モデルによる出力値が分類結果に対応するよう、モデルを学習させることになる。この場合、モデルのことを分類器とも呼ぶ。
機械学習において、素性への変換部分は、データの種類に大きく依存する。一方、分類器に用いる数学的モデルと、モデルの改修法、つまり学習法は、汎用的に使うことができる。あるいは、画像や音声、文章といったデータの多様性を、素性という一般的な数値に落とし込むことで吸収して、汎用的分類モデルでも学習できるようにしている。
Deep Learning、あるいはDeep Neural Network(多層ニューラルネットワーク)は、汎用的分類モデルの一種である。ここでは、Deep Learningの他にどのような分類器が存在するのか、代表的なものを述べる。
2.2.1 Support Vector Machine
Support Vector Macine(SVM)は、データを2つのクラスに分類する能力を持っている。
カーネルマジックとマージン最大化により、優れた精度を出している。SVMは、画像認識などに既に実用化されている。[加筆]
SVMは、広くその信頼性が認められたモデルの1つであり、ライブラリの利用方法も確立している。SVMを簡単に使えるようにlibsvmやliblinearというライブラリが存在している。これらのライブラリを使うと、簡単な所定の方式に沿って入力データファイルを用意し、CUI上で2,3回の操作をするだけで、SVMによる分類を行わせることができるライブラリである。このとき、利用者が自分でプログラムを書く必要は全くない。プログラムを書かないで済むと、手軽に利用することができ、またバグを起こす危険性が非常に少なく安全に使うことができる。Deep Learningについては、このようなライブラリはまだ存在していないため、Deep Learningの代表的なアルゴリズムについて、プログラム無しで利用できるようなライブラリの整備が望まれる。
2.2.2 ニューラルネットワーク
ニューラルネットワークは、人間の脳の構造を模倣した数学的モデルである。人間の脳は、ニューロンと呼ばれる神経細胞が大量に接続されて出来ている。ニューロンが電気信号を伝達することで、様々な脳の働きが行われている、と考えられている。[模式図など加筆]
・隠れ層の追加により、任意の非線形関数を近似可能
・バックプロパゲーションの弱点(sigmoidが反応しなくなる)
* 第3章　Deep Learningのアルゴリズム
** 3.1 Deep Learningの歴史
Deep Learningの登場 Hinton, Bengio, LeCun
画像認識タスクでの成績、音声認識タスクでの成果、猫認識
MNIST, CIFAR, SVHNなどにおけるconv.net、Maxout, DropConnectの優位
** 3.2 Deep Learningモデルのバリエーションと詳細
Deep Learningの
・SDA, DBM, CNN
・Dropout, Maxout, DropConnect
rectifier, pretraningとfinetuning
* 第4章　Deep Learningの実装における技術
** 4.1 評価基準
Deep Learningのアルゴリズムを使用するための具体的な手段を選ぶにあたり、次のような判断基準を設けた。優先順位は、1→2→3の順に高い。
基準1. Deep Learningが注目された大きな理由である、高い識別精度を再現できる。
基準2. 学習にかかる時間が、他のアルゴリズムに比べて、極端に長くならない。
基準3. 利用にあたって必要なプログラミング量が出来るだけ少なく、バグが混入しにくい。
基準1は最優先目標である。今回Deep Learningを選択した理由は、Web工学のタスクにおいて、高い分類精度を実現するための最も有力な方法だと思われるからである。つまり、例えDeep Learningのアルゴリズムとして正しいプログラムだったとしても、分類精度が従来の分類器より劣っていれば意味はない。従来の分類器をそのまま使い続ければよいことになってしまう。
基準2と3は、現実の問題をDeep Learningで解決する際に、必要となってくる視点である。プログラムの作成や、学習の実行にかかる時間が、あまりにも長くなってしまうと、実際のビジネスや、刻一刻と変化するWebサービスに対して応用するのは非現実的だろう。
** 4.2 既存ソースコード使用の利点
Deep Learningのプログラムを実行するために、まず大きく分けて、「自分でソースコードを全て書く」方法と、「主に既存のソースコードを利用する」方法の2つが考えられる。今回は、既存ソースコードをベースに使うことを選択した。以下、その理由を説明する。
「既存のソースコードを利用する」場合のメリットとして、「開発期間は基本的にゼロで済む」「新たにバグが混入する危険性が無い」「自分の改造コードが、他のライブラリ利用者によって使ってもらえるチャンスが大きい」という点が挙げられる。デメリットとして、「ソースコード中にブラックボックスが増え、改造にかかる時間が短くなる」「全く新しいアルゴリズムを実装する場合、ゼロからスタートした方が早く書けるケースもある」などが想定される。しかし、基準3の「プログラミング量が少なくバグのリスクが低い」という点において優れているため、今回は既存のソースコードを探して利用していくことにした。
なお、「自分でソースコードを全て書く」場合のメリットとデメリットは、上記「既存のソースコード」の場合の逆となる。自分で新たなコードを書くので時間がかかり、バグの混入リスクも大きい。また、ソースコードを公開した場合の、ライブラリとしての信頼度も、ゼロから築かなければならない。ただし、コードの詳細部分を改造する段階では、自分の手で書いたコードを使う方が、より深い理解を得やすく、確実で素早い実装ができる可能性もある。
** 4.3 GPUの利用による高速化
一般に、Deep Learningの研究においては、GPGPUによる並列計算が有効とされている。並列計算のプログラム構成にもよるが、100倍近く早くなることもある。また、Deep Learningの実装によっては、はじめからGPUで高速化することを前提に、GPU専用のコードを書いている場合があるこの場合、そもそもGPUを搭載したマシンを使わないと、コンパイルや実行が全く出来なくなってしまう。
** 4.4 数値計算ライブラリTheanoの利点
Theanoは、python上で記述される数式処理/数値計算ライブラリである。Theanoは、数式のコンパイルと実際のデータによる数値計算の2段階で動作する。数式は文字式で記述される。このときデバッグの難しい数値計算の誤差絡みの部分、0divなど危険な処理を、自動的に解析してくれる。ニューラルネットワークの演算過程では、非常に小さい数値が出現することがあり、また、文字式を分析することで、計算グラフも最適化してくれる。プログラマは、プログラム上の些末な計算テクニックに囚われることなく、数式の本質的な部分の記述に集中することができる。また、微分も自動で行ってくれる。ニューラルネットワークでは、様々な文字の微分が必要になり、多層にすると爆発的に式が複雑になる。これを手計算による文字式の微分をしてから書いていくと、層を増やす度に手計算が必要になり、非常に煩雑で、開発効率が落ちてしまう。Theanoの自動微分と自動最適化は、pylearn2の内部記述の簡略化を大いに助けているおいて、多層のニューラルネットワークを使うときでも、文字式に対する大量の手計算と、大量のプログラムの式を書くことなく、簡単に微分やニューラルネットワークを拡張することができる。
** 4.5 機械学習ライブラリPylearn2の利点
今回は、Deep Learningを使うための既存ソースコードとして、モントリオール大学のLISA Lab.が提供している、pylearn2というライブラリを選んだ。pylearn2は、pythonで記述された、Deep Learningなど機械学習アルゴリズムを使うためのライブラリである。LISA Lab.を中心とする開発者によって、ソースコードの開発がほぼ毎日行われている。以下、pylearn2を選択した理由を記す。
4.5.1 高精度アルゴリズムMaxout Networkの実装
pylearn2には、Maxout NetworkというDeep Learningのアルゴリズムの一種が実装されている。このアルゴリズムは、画像認識タスクにおいて非常に高い精度を実現しており、基準1を満たす上で都合が良い。
Maxout Networkの解説[過去のスライドをもとに加筆]
4.5.2 Theanoの利用
pylearn2のプログラムは、前述したTheanoを全面的に利用して記述されている。Theanoを用いたことで、可読性が大きく上昇している。また、何らかの理由で、pylearn2の部分的拡張が必要になったときでも、[過去のスライドを基に加筆]
4.5.3 高度な拡張性と可読性
pylearn2のソースコードは、拡張性や可読性を非常に強く意識して書かれている。基本的な使い方を記したチュートリアルのWebページが存在し、主なソースファイルには、詳細なドキュメントも記述されている。また、モジュール化が丁寧なので、自分で書く部分が少なくて済む。例えば、データセットを変更する場合は、Datasetクラスのみを書き下せばよく、他の部分に対してコードを書く必要がほとんどない。また、プログラムを改造する場合でも、数値パラメータを変更するだけであれば、設定ファイルのみの変更で完結させることができる。この設定ファイルは、YAML(YAML Ain't a Markup Language)形式に少し独自拡張を加えた形式になっている。
プログラム本体と、数値の設定ファイルが分離されていることで、プログラミングが不得手な人でも、比較的簡単に設定を変更することができる。
** 4.6 ハードウェアの構成
pylearn2を通してDeep Learningのアルゴリズムを実行する、と決めたところで、実際にDeep Learningを利用するためのマシンを用意する必要がある。
まず、Deep Learningに限った話ではないが、機械学習のタスクは長時間の計算を必要とする場合が多く、プログラムの実行時間が1日以上かかることも稀ではない。普段使用しているPCが使えなくなると困る場合は、機械学習を動かしておくための専用サーバを用意することが望ましい。
GPUは必ずしも搭載していなくとも良いのだが、実践上TheanoやCUDAの性能を最大限に生かすためには、GPUを搭載しなければならない。また、CUDAを用いてGPUマシン専用に書かれたアルゴリズムを動作させるためには、GPUが必須となる。GPUは、NVIDIA製で、CUDAに対応していなければならない。ただし、グラフィック出力の機能はなくても構わない。2013年1月現在、NVIDIAのWebページに掲載されているGPUに関しては、全てCUDAに対応しているため、店舗で新品のGPUを購入する場合は基本的に問題は生じないと思われる。
NVIDIAのGPUの中で、どれを選ぶかも問題になる。NVIDIAのホームページでは、GPUのうちTeslaというシリーズがGPGPUに特化しており、非常に高精度/高速な演算を行うことができる、と述べられている。しかし、非常に高価な上、Teslaシリーズは一般的なグラフィックカードではなく、サーバ全体の購入を基本としている。販売員の方によれば、Teslaは主に商用の大規模データや、ミスの許されない長時間科学計算などに用いることを想定して作られている。今回の目的は、Web工学の一般的タスクにおけるDeep Learning技術を確立するという、いわば実験的な用途であり、Teslaの利用はオーバースペックだと思われる。
NVIDIAのGPUは、Teslaを除くと、QuadroとGeForceという2つのシリーズに分かれている。店舗の販売員の方に詳しい話を伺ったところ、Quadroはコンピュータグラフィックの出力機能に注力したシリーズであり、GeForceはより計算性能重視という傾向をもっている。つまり、Deep Learningの計算などGPGPUに用いる場合、同じ価格帯ならば、GeForceシリーズのGPUを用いた方が、費用対効果が大きくなると考えられる。
また、販売員の方によれば、GeForceシリーズの中でもTitanという機種は、Tesla用の部品の中で品質チェックに漏れてしまったものを流用しており、Teslaとほぼ同様の、非常に高い性能を発揮することが出来る、とのことだった。しかし、今回利用する中で最も計算量が大きいMaxout Networkについて、元の発表論文によれば、GeForceシリーズのGTX 580という、少し世代が前のGPUが用いられている。Deep Learningを実行させる上で、GPUが存在すること自体は非常に重要だが、必ずしも最新スペックのGPUが必要というわけではないことがわかる。
今回の構成では、知の構造化センターの中山浩太郎先生のアドバイスもあり、Titanではなく、同じGeForceシリーズのGTX 760というGPUを搭載することにした。
* 第5章　実験と結果
この章では、第4章で得られたDeep Learning用の構成を用いて、実際に機械学習のタスクを実行する。分類精度、実行時間、消費メモリ量を調べることにより、Deep LearningをWeb工学の問題に適用する際の参考とする。
** 5.1 利用したデータセット
機械学習の分野では、分類精度のベンチマークを取るために、様々なデータセットが提供/提案されてきている。同じデータセットに対して、様々な分類モデルやアルゴリズムを用いて分類実験を行うことにより、どの手法が優れているのか比較することが出来る。
ここでは、画像認識のデータセットを用いて、Deep Learningプログラムのベンチマークを行う。画像データを用いる理由は、1つには、画像認識がDeep Learningが最も高い分類性能を実現している分野だからである。加えて、画像データや画像から抽出された素性は、可視化が比較的容易なことが多い。可視化することで、学習過程を目で見て確かめることが出来るため、アルゴリズムの分析を行いやすい、という利点がある。
5.1.1 MNIST
MNISTは、手書き数字を画像分析によって認識するベンチマークタスクである。28x28の画像に、0〜9までの数字がどれか1つ、手書きされている。これを読み取って、どのくらい高い精度で分類できるかによって、アルゴリズムの分類精度を示すことになる。[加筆、図追加]
5.2.2 CIFAR10
CIFAR10は、写真を画像分析によって識別するタスクである。32x32ピクセルの画像が60000個、入力データとして与えられる。60000の写真画像はいずれも、飛行機、自動車、犬などの10種類のクラスのうちどれかに該当している。これらの写真データを読み取って、どれほど高い精度で正しいクラスに分類することができるかを競う。[加筆、図追加]
** 5.2 Deep Learningのライブラリに対するベンチマーク実験
5.3.1 Maxout Networkによる、MNISTの分類タスク(2次元データとして扱う)
Maxout Networkを分類器として用い、MNISTの分類タスクを行わせた。モデル構造としては、Convolutional Layerを2層このアルゴリズムが発表された論文によれば、このモデル分類誤差0.45%を元画像に対してstate of the art
実験結果では、誤差0.51%となり、元の論文が主張している精度より、少し悪い結果となった。[加筆、表追加]

5.3.2 Maxout Networkによる、MNISTの分類タスク(1次元データとして扱う)
5.2.1と同じタスクを、「MNISTのデータは、2次元の画像データではなく、1次元のベクトルデータである」という条件の基で行わせた。つまり、2次元データに対するConvolutional Layer技術を敢えて使わない状態で、どれだけの精度をMaxout Networkが実現できるのか、実験した。元論文によれば、0.94%の精度が実現されるはずだが、実際に実行したところ、誤差1.16%と、やはり少し低い精度になってしまった。

5.3.3 Maxout Networkによる、CIFAR10の分類タスク
(時間が10日以上かかることがわかり、中断したことを書く)

** 5.3 ソーシャルメディアにおけるプロフィール画像の識別実験
(夏の実験について記す)

* 第6章　考察と提言
** 6.1 考察
・pylearn2におけるMaxout Networkは、元論文に記されている精度こそ再現できないが、MNISTの分類タスクにおいてState of the Artに近い精度を実現することが出来た。
・精度が悪くなった理由として、複数回実行によるばらつきがなく、ハードウェアの性能も今回実験に使った方が優れている。numpyやTheanoのバージョン違いにより、再現性が下がっていると考えるべきである。
・CIFAR10の実行時間が長くかかるのは必要不可欠なのか、それとも短縮する方法があるのか、検証する必要がある。
** 6.2 Deep Learningの利用法に関する提言
・Maxout Networkが良い精度を実現できたこと、改造のしやすさ、GPU利用の簡便さなどを考え合わせると、現時点では、pylearn2を通してDeep Learningを利用するのが良いと考えられる。
* 第7章　おわりに
** 7.1 研究の成果
この研究では、Deep LearningをWeb工学の問題に適用するにあたって、その特徴である高い精度を落とすことなく、出来るだけ簡便に応用するための方法論とノウハウを調査した。pylearn2を用い、Datasetクラスのみを書き換えて分類すると良い。GPUを使うと大幅な高速化が見込めるが、必須ではない。
** 7.2 今後の課題
画像分類タスクにおいて有効な方法が、Web工学のデータでも必ず有効かどうかは、未知数な部分がある。例えば、Convolutional Networkは、2次元の画像データに対しては非常に高い効果を挙げるが、そのまま文章データに応用することは出来ない。pylearn2で言えば、Datasetクラスを作るだけで識別がうまくいくのか、文章専用のModelを構成する必要があるのか、確かめていく必要がある。
また、精度を上げるためには、どのようにハイパーパラメータを調整すれば良いのか、あるいは精度を多少犠牲にしてでも、比較的短い実行時間で良い結果を得たい時、どのような調整を施せばよいのかは、まだわかっておらず、今後の大きな課題の一つである。
Deep Learningを試す上で、大きなネックとなるのが、CUDAを用いたGPU専用のソースコードの存在である。GPUを搭載していなかったり、使えるメモリが少ない状況下でも、Deep Learningのコードを効率良く動かすことが出来れば、Deep Learningの利便性はますます増加するだろう。
現在のDeep Learningでは、画像のフィルタで何が学習されているのかを、部分的に可視化することはできる。しかし、文書解析において、どのような表現を学習したのかを、人間に理解できる形でみることは難しい。言い換えれば、学習によって、分類器がどこに注目するようになったのか、文構造や、感情、文体などに対応するニューロンが存在しているのか、といった情報が、人間に理解できる形になっていない。この部分をどうやって可視化して、人間に理解できる状態にするか、そこからどのような知見を得られるのか、あるいはそもそも人間には理解出来る知識として取り出せるのかどうか、といったことを調べることにより、表現学習としてのDeep Learningの側面を、さらに活かすことができると思われる。例えば、Deep Learningにょって学習された、データの着目点や抽象化のポイントを、人間が真似することによって、人間の方が機械から知識を習得し、さらに発展させることも考えられる。
* 謝辞
本研究を進めるにあたり、工学系研究科准教授の松尾豊先生には、研究の方針や基本的な進め方に始まり、論文の構成法など様々な面で手厚い指導をして頂きました。知の構造化センター特任講師の中山浩太郎先生には、GPU搭載マシンの構成、Linuxサーバの構築、CUDAプログラムのコンパイル環境セットアップなど、実装面において多岐にわたるアドバイスを頂きました。情報理工学系研究科講師の中山英樹先生には、画像処理の分野における一般的な前提知識について、多くの示唆を頂きました。また、松尾研究室博士課程の大澤さんは、毎週貴重な時間を割いて、研究に関するディスカッションを実施して下さった上、私の未熟なプレゼン技術を丁寧に指導して下さいました。松尾研究室修士課程の飯塚さんと、研究生の那須野さんは、サーバ管理者として、お忙しい中サーバのセッティングを助けてくださいました。松尾研究室学部4年の川上さんは、Deep Learning研究グループのメンバーとして、数々の助言と情報をくださいました。また、ここに一人一人名を挙げることは出来ませんが、松尾研究室の皆様方には、研究会を中心として、様々なアドバイスを頂きました。本研究は多くの人の協力によって成り立っており、ここに謝辞を述べたいと思います。ありがとうございました。
東京大学工学部システム創成学科 
知能社会システムコース
松尾研究室 4年 黒滝 紘生
平成26年2月
* 参考文献
[Goldberg 92] David Goldberg, David Nichols, Brian M. Oki and Douglas Terry : Using collaborative filtering to weave an information Tapestry, Communications of the ACM, Vol. 35, No. 12, 1992.
[Resnick 97] Paul Resnick, Hal R.Varian, Guest Editors : Recommender Systems, Communications of the ACM, Vol. 40, No. 3, 1997.
[Schafer 99] J. Ben Schafer, Joseph Konstan, John Riedl : Recommender Systems in E-Commerce, Proceedings of the 1st ACM conference on Electronic commerce, 1999
[Koren 2009] Yehuda Koren, Robert Bell, Chris Volinsky : Matrix Factorization Techniques for Recommender Systems, Journal Computer, Vol. 42, Issue 8, 2009

